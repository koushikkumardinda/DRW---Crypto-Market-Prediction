{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5eb1f4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-21T02:24:47.074545Z",
     "iopub.status.busy": "2025-07-21T02:24:47.074336Z",
     "iopub.status.idle": "2025-07-21T02:24:48.634922Z",
     "shell.execute_reply": "2025-07-21T02:24:48.634132Z"
    },
    "papermill": {
     "duration": 1.565315,
     "end_time": "2025-07-21T02:24:48.636118",
     "exception": false,
     "start_time": "2025-07-21T02:24:47.070803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\n",
      "/kaggle/input/drw-crypto-market-prediction/train.parquet\n",
      "/kaggle/input/drw-crypto-market-prediction/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba792dcb",
   "metadata": {
    "papermill": {
     "duration": 0.002179,
     "end_time": "2025-07-21T02:24:48.641150",
     "exception": false,
     "start_time": "2025-07-21T02:24:48.638971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7651f908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T02:24:48.646570Z",
     "iopub.status.busy": "2025-07-21T02:24:48.646276Z",
     "iopub.status.idle": "2025-07-21T02:25:03.930397Z",
     "shell.execute_reply": "2025-07-21T02:25:03.929839Z"
    },
    "papermill": {
     "duration": 15.288316,
     "end_time": "2025-07-21T02:25:03.931727",
     "exception": false,
     "start_time": "2025-07-21T02:24:48.643411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from scipy.stats import pearsonr\n",
    "import cudf # For GPU-accelerated dataframes\n",
    "import cupy as cp # For GPU-accelerated numpy operations\n",
    "import lightgbm as lgb # A strong baseline/ensemble candidate, even without explicit time features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768a82d",
   "metadata": {
    "papermill": {
     "duration": 0.002365,
     "end_time": "2025-07-21T02:25:03.936808",
     "exception": false,
     "start_time": "2025-07-21T02:25:03.934443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Data Loading and Preprocessing (Memory Efficient) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b712591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T02:25:03.942711Z",
     "iopub.status.busy": "2025-07-21T02:25:03.942419Z",
     "iopub.status.idle": "2025-07-21T02:25:40.419158Z",
     "shell.execute_reply": "2025-07-21T02:25:40.418566Z"
    },
    "papermill": {
     "duration": 36.48129,
     "end_time": "2025-07-21T02:25:40.420418",
     "exception": false,
     "start_time": "2025-07-21T02:25:03.939128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into cuDF successfully!\n",
      "Shape of train_df: (525886, 787)\n",
      "Shape of test_df: (538150, 786)\n"
     ]
    }
   ],
   "source": [
    "# Function to reduce memory usage of a DataFrame (for pandas if you choose to use it for some steps)\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "    print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.2f}%')\n",
    "    return df\n",
    "\n",
    "# Load data with Polars first for efficiency, then convert to cuDF\n",
    "# Or directly with cuDF if the file is not too large for direct cuDF read_parquet\n",
    "try:\n",
    "    train_df_pl = pl.read_parquet(\"/kaggle/input/drw-crypto-market-prediction/train.parquet\")\n",
    "    test_df_pl = pl.read_parquet(\"/kaggle/input/drw-crypto-market-prediction/test.parquet\")\n",
    "    \n",
    "    # Convert Polars DataFrames to cuDF DataFrames for GPU operations\n",
    "    train_df = cudf.DataFrame(train_df_pl.to_pandas())\n",
    "    test_df = cudf.DataFrame(test_df_pl.to_pandas())\n",
    "\n",
    "    del train_df_pl, test_df_pl\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Data loaded into cuDF successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data with cuDF/Polars, trying pandas: {e}\")\n",
    "    # Fallback to pandas with memory reduction if cuDF direct read or conversion fails due to size\n",
    "    train_df = pd.read_parquet(\"train.parquet\")\n",
    "    test_df = pd.read_parquet(\"test.parquet\")\n",
    "    train_df = reduce_mem_usage(train_df)\n",
    "    test_df = reduce_mem_usage(test_df)\n",
    "    print(\"Data loaded into pandas with memory reduction.\")\n",
    "    # If using pandas, move to CuPy/PyTorch tensors when needed to leverage GPU\n",
    "\n",
    "# Define features and target\n",
    "features = [col for col in train_df.columns if 'X' in col]\n",
    "target = 'label'\n",
    "\n",
    "# Handle potential NaN values (e.g., fill with median or mean for numerical features)\n",
    "# For cuDF, use .fillna()\n",
    "for col in features:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "    if col in test_df.columns:\n",
    "        test_df[col] = test_df[col].fillna(test_df[col].median())\n",
    "    \n",
    "# Convert target to float32\n",
    "if target in train_df.columns:\n",
    "    train_df[target] = train_df[target].astype(np.float32)\n",
    "\n",
    "print(f\"Shape of train_df: {train_df.shape}\")\n",
    "print(f\"Shape of test_df: {test_df.shape}\")\n",
    "\n",
    "# Drop 'timestamp' from train_df if present, as it's not useful for direct time-series in test\n",
    "if 'timestamp' in train_df.columns:\n",
    "    train_df = train_df.drop('timestamp', axis=1)\n",
    "if 'timestamp' in test_df.columns: # test timestamp is masked anyway\n",
    "    test_df = test_df.drop('timestamp', axis=1)\n",
    "\n",
    "# Ensure all features are float32 for GPU efficiency\n",
    "for col in features:\n",
    "    train_df[col] = train_df[col].astype(np.float32)\n",
    "    test_df[col] = test_df[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65045a",
   "metadata": {
    "papermill": {
     "duration": 0.002305,
     "end_time": "2025-07-21T02:25:40.425465",
     "exception": false,
     "start_time": "2025-07-21T02:25:40.423160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "259ce89c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T02:25:40.431154Z",
     "iopub.status.busy": "2025-07-21T02:25:40.430934Z",
     "iopub.status.idle": "2025-07-21T02:25:41.525443Z",
     "shell.execute_reply": "2025-07-21T02:25:41.524665Z"
    },
    "papermill": {
     "duration": 1.098882,
     "end_time": "2025-07-21T02:25:41.526723",
     "exception": false,
     "start_time": "2025-07-21T02:25:40.427841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering complete.\n"
     ]
    }
   ],
   "source": [
    "# Simple feature engineering examples (can be expanded)\n",
    "# Note: cuDF handles many pandas-like operations\n",
    "# If you have specific domain knowledge about these 'X' features, you can create more tailored ones.\n",
    "\n",
    "# Example: Mean and Std Dev across related features (if X1-X10 represent a group)\n",
    "# This is a general example and might need adjustment based on feature relationships\n",
    "# For instance, if X1-X5 are bid-related, X6-X10 are ask-related.\n",
    "# For simplicity, let's create a few interaction terms and basic statistics.\n",
    "\n",
    "# Note: These operations can be memory intensive, be selective.\n",
    "# Using CuPy for complex numpy-like operations on GPU arrays can be beneficial.\n",
    "\n",
    "def feature_engineer(df, features_list):\n",
    "    # Convert to CuPy array for faster operations if df is cuDF\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        df_cp = df[features_list].to_cupy()\n",
    "    else: # If using pandas\n",
    "        df_cp = cp.asarray(df[features_list].values)\n",
    "\n",
    "    # Example: Simple sum and mean of all X features\n",
    "    df['sum_X'] = cp.sum(df_cp, axis=1)\n",
    "    df['mean_X'] = cp.mean(df_cp, axis=1)\n",
    "    df['std_X'] = cp.std(df_cp, axis=1)\n",
    "    \n",
    "    # You can add more complex interactions here if RAM permits\n",
    "    # e.g., ratios, differences, polynomial features for selected important features\n",
    "    # df['X1_div_X2'] = df['X1'] / (df['X2'] + 1e-6) # Avoid division by zero\n",
    "\n",
    "    # Convert back to cuDF if originally cuDF, or to pandas if originally pandas\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        df['sum_X'] = df['sum_X'].astype(np.float32)\n",
    "        df['mean_X'] = df['mean_X'].astype(np.float32)\n",
    "        df['std_X'] = df['std_X'].astype(np.float32)\n",
    "    else:\n",
    "        df['sum_X'] = df['sum_X'].get().astype(np.float32) # .get() to move from CuPy to NumPy\n",
    "        df['mean_X'] = df['mean_X'].get().astype(np.float32)\n",
    "        df['std_X'] = df['std_X'].get().astype(np.float32)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = feature_engineer(train_df, features)\n",
    "test_df = feature_engineer(test_df, features)\n",
    "\n",
    "# Update features list to include new engineered features\n",
    "features.extend(['sum_X', 'mean_X', 'std_X'])\n",
    "\n",
    "print(\"Feature Engineering complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01aec40",
   "metadata": {
    "papermill": {
     "duration": 0.002318,
     "end_time": "2025-07-21T02:25:41.531725",
     "exception": false,
     "start_time": "2025-07-21T02:25:41.529407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Scaling and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da28eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T02:25:41.537870Z",
     "iopub.status.busy": "2025-07-21T02:25:41.537259Z",
     "iopub.status.idle": "2025-07-21T02:26:02.082851Z",
     "shell.execute_reply": "2025-07-21T02:26:02.082016Z"
    },
    "papermill": {
     "duration": 20.549906,
     "end_time": "2025-07-21T02:26:02.084134",
     "exception": false,
     "start_time": "2025-07-21T02:25:41.534228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data converted to NumPy arrays.\n",
      "Data scaled and converted to CuPy arrays.\n"
     ]
    }
   ],
   "source": [
    "# Use StandardScaler for normalization. QuantileTransformer can also be effective.\n",
    "# For large datasets, it's common to fit the scaler on a subset or in batches\n",
    "# if RAM is very limited, but for numerical features, standard scaling is usually fine.\n",
    "# Note: scikit-learn's scalers are CPU-based. If using cuDF, convert to NumPy then scale, or use RAPIDS equivalents.\n",
    "\n",
    "X_train = train_df[features].to_pandas().values if isinstance(train_df, cudf.DataFrame) else train_df[features].values\n",
    "y_train = train_df[target].to_pandas().values if isinstance(train_df, cudf.DataFrame) else train_df[target].values\n",
    "X_test = test_df[features].to_pandas().values if isinstance(test_df, cudf.DataFrame) else test_df[features].values\n",
    "\n",
    "# Free up memory\n",
    "del train_df, test_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"Data converted to NumPy arrays.\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to CuPy arrays for GPU processing with PyTorch\n",
    "X_train_scaled = cp.asarray(X_train_scaled, dtype=cp.float32)\n",
    "X_test_scaled = cp.asarray(X_test_scaled, dtype=cp.float32)\n",
    "y_train = cp.asarray(y_train, dtype=cp.float32)\n",
    "\n",
    "print(\"Data scaled and converted to CuPy arrays.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b38d78",
   "metadata": {
    "papermill": {
     "duration": 0.002395,
     "end_time": "2025-07-21T02:26:02.089369",
     "exception": false,
     "start_time": "2025-07-21T02:26:02.086974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Advanced Model: Deep Neural Network (DNN) with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62a069a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T02:26:02.095605Z",
     "iopub.status.busy": "2025-07-21T02:26:02.095063Z",
     "iopub.status.idle": "2025-07-21T02:32:55.926093Z",
     "shell.execute_reply": "2025-07-21T02:32:55.925417Z"
    },
    "papermill": {
     "duration": 413.835536,
     "end_time": "2025-07-21T02:32:55.927343",
     "exception": false,
     "start_time": "2025-07-21T02:26:02.091807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "Epoch 1/15, Loss: 0.6512, Val Pearson Corr: 0.7287\n",
      "New best model saved with correlation: 0.7287\n",
      "Epoch 2/15, Loss: 0.3506, Val Pearson Corr: 0.8655\n",
      "New best model saved with correlation: 0.8655\n",
      "Epoch 3/15, Loss: 0.2745, Val Pearson Corr: 0.9098\n",
      "New best model saved with correlation: 0.9098\n",
      "Epoch 4/15, Loss: 0.2320, Val Pearson Corr: 0.9283\n",
      "New best model saved with correlation: 0.9283\n",
      "Epoch 5/15, Loss: 0.2258, Val Pearson Corr: 0.9453\n",
      "New best model saved with correlation: 0.9453\n",
      "Epoch 6/15, Loss: 0.1854, Val Pearson Corr: 0.9512\n",
      "New best model saved with correlation: 0.9512\n",
      "Epoch 7/15, Loss: 0.1620, Val Pearson Corr: 0.9586\n",
      "New best model saved with correlation: 0.9586\n",
      "Epoch 8/15, Loss: 0.1486, Val Pearson Corr: 0.9590\n",
      "New best model saved with correlation: 0.9590\n",
      "Epoch 9/15, Loss: 0.1516, Val Pearson Corr: 0.9643\n",
      "New best model saved with correlation: 0.9643\n",
      "Epoch 10/15, Loss: 0.1570, Val Pearson Corr: 0.9659\n",
      "New best model saved with correlation: 0.9659\n",
      "Epoch 11/15, Loss: 0.1300, Val Pearson Corr: 0.9708\n",
      "New best model saved with correlation: 0.9708\n",
      "Epoch 12/15, Loss: 0.1338, Val Pearson Corr: 0.9705\n",
      "Epoch 13/15, Loss: 0.1270, Val Pearson Corr: 0.9724\n",
      "New best model saved with correlation: 0.9724\n",
      "Epoch 14/15, Loss: 0.1332, Val Pearson Corr: 0.9722\n",
      "Epoch 15/15, Loss: 0.1210, Val Pearson Corr: 0.9728\n",
      "New best model saved with correlation: 0.9728\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "Epoch 1/15, Loss: 0.5569, Val Pearson Corr: 0.6991\n",
      "New best model saved with correlation: 0.6991\n",
      "Epoch 2/15, Loss: 0.3467, Val Pearson Corr: 0.8620\n",
      "New best model saved with correlation: 0.8620\n",
      "Epoch 3/15, Loss: 0.2647, Val Pearson Corr: 0.9090\n",
      "New best model saved with correlation: 0.9090\n",
      "Epoch 4/15, Loss: 0.2346, Val Pearson Corr: 0.9258\n",
      "New best model saved with correlation: 0.9258\n",
      "Epoch 5/15, Loss: 0.1913, Val Pearson Corr: 0.9420\n",
      "New best model saved with correlation: 0.9420\n",
      "Epoch 6/15, Loss: 0.1621, Val Pearson Corr: 0.9523\n",
      "New best model saved with correlation: 0.9523\n",
      "Epoch 7/15, Loss: 0.2012, Val Pearson Corr: 0.9564\n",
      "New best model saved with correlation: 0.9564\n",
      "Epoch 8/15, Loss: 0.1373, Val Pearson Corr: 0.9625\n",
      "New best model saved with correlation: 0.9625\n",
      "Epoch 9/15, Loss: 0.1448, Val Pearson Corr: 0.9655\n",
      "New best model saved with correlation: 0.9655\n",
      "Epoch 10/15, Loss: 0.1274, Val Pearson Corr: 0.9676\n",
      "New best model saved with correlation: 0.9676\n",
      "Epoch 11/15, Loss: 0.1620, Val Pearson Corr: 0.9682\n",
      "New best model saved with correlation: 0.9682\n",
      "Epoch 12/15, Loss: 0.1329, Val Pearson Corr: 0.9704\n",
      "New best model saved with correlation: 0.9704\n",
      "Epoch 13/15, Loss: 0.1178, Val Pearson Corr: 0.9716\n",
      "New best model saved with correlation: 0.9716\n",
      "Epoch 14/15, Loss: 0.1293, Val Pearson Corr: 0.9707\n",
      "Epoch 15/15, Loss: 0.1111, Val Pearson Corr: 0.9758\n",
      "New best model saved with correlation: 0.9758\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "Epoch 1/15, Loss: 0.5277, Val Pearson Corr: 0.7251\n",
      "New best model saved with correlation: 0.7251\n",
      "Epoch 2/15, Loss: 0.3894, Val Pearson Corr: 0.8606\n",
      "New best model saved with correlation: 0.8606\n",
      "Epoch 3/15, Loss: 0.2955, Val Pearson Corr: 0.9099\n",
      "New best model saved with correlation: 0.9099\n",
      "Epoch 4/15, Loss: 0.2018, Val Pearson Corr: 0.9309\n",
      "New best model saved with correlation: 0.9309\n",
      "Epoch 5/15, Loss: 0.1895, Val Pearson Corr: 0.9423\n",
      "New best model saved with correlation: 0.9423\n",
      "Epoch 6/15, Loss: 0.2047, Val Pearson Corr: 0.9493\n",
      "New best model saved with correlation: 0.9493\n",
      "Epoch 7/15, Loss: 0.1611, Val Pearson Corr: 0.9574\n",
      "New best model saved with correlation: 0.9574\n",
      "Epoch 8/15, Loss: 0.1540, Val Pearson Corr: 0.9598\n",
      "New best model saved with correlation: 0.9598\n",
      "Epoch 9/15, Loss: 0.1497, Val Pearson Corr: 0.9636\n",
      "New best model saved with correlation: 0.9636\n",
      "Epoch 10/15, Loss: 0.1676, Val Pearson Corr: 0.9661\n",
      "New best model saved with correlation: 0.9661\n",
      "Epoch 11/15, Loss: 0.1338, Val Pearson Corr: 0.9661\n",
      "Epoch 12/15, Loss: 0.1096, Val Pearson Corr: 0.9696\n",
      "New best model saved with correlation: 0.9696\n",
      "Epoch 13/15, Loss: 0.1651, Val Pearson Corr: 0.9717\n",
      "New best model saved with correlation: 0.9717\n",
      "Epoch 14/15, Loss: 0.1269, Val Pearson Corr: 0.9719\n",
      "New best model saved with correlation: 0.9719\n",
      "Epoch 15/15, Loss: 0.1075, Val Pearson Corr: 0.9745\n",
      "New best model saved with correlation: 0.9745\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "Epoch 1/15, Loss: 0.7104, Val Pearson Corr: 0.7077\n",
      "New best model saved with correlation: 0.7077\n",
      "Epoch 2/15, Loss: 0.3688, Val Pearson Corr: 0.8602\n",
      "New best model saved with correlation: 0.8602\n",
      "Epoch 3/15, Loss: 0.2602, Val Pearson Corr: 0.9128\n",
      "New best model saved with correlation: 0.9128\n",
      "Epoch 4/15, Loss: 0.2263, Val Pearson Corr: 0.9322\n",
      "New best model saved with correlation: 0.9322\n",
      "Epoch 5/15, Loss: 0.1989, Val Pearson Corr: 0.9458\n",
      "New best model saved with correlation: 0.9458\n",
      "Epoch 6/15, Loss: 0.2240, Val Pearson Corr: 0.9529\n",
      "New best model saved with correlation: 0.9529\n",
      "Epoch 7/15, Loss: 0.1658, Val Pearson Corr: 0.9538\n",
      "New best model saved with correlation: 0.9538\n",
      "Epoch 8/15, Loss: 0.1712, Val Pearson Corr: 0.9615\n",
      "New best model saved with correlation: 0.9615\n",
      "Epoch 9/15, Loss: 0.1659, Val Pearson Corr: 0.9632\n",
      "New best model saved with correlation: 0.9632\n",
      "Epoch 10/15, Loss: 0.1312, Val Pearson Corr: 0.9680\n",
      "New best model saved with correlation: 0.9680\n",
      "Epoch 11/15, Loss: 0.1189, Val Pearson Corr: 0.9677\n",
      "Epoch 12/15, Loss: 0.1132, Val Pearson Corr: 0.9711\n",
      "New best model saved with correlation: 0.9711\n",
      "Epoch 13/15, Loss: 0.1254, Val Pearson Corr: 0.9714\n",
      "New best model saved with correlation: 0.9714\n",
      "Epoch 14/15, Loss: 0.1146, Val Pearson Corr: 0.9725\n",
      "New best model saved with correlation: 0.9725\n",
      "Epoch 15/15, Loss: 0.1191, Val Pearson Corr: 0.9698\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "Epoch 1/15, Loss: 0.5248, Val Pearson Corr: 0.7163\n",
      "New best model saved with correlation: 0.7163\n",
      "Epoch 2/15, Loss: 0.3899, Val Pearson Corr: 0.8676\n",
      "New best model saved with correlation: 0.8676\n",
      "Epoch 3/15, Loss: 0.2574, Val Pearson Corr: 0.9019\n",
      "New best model saved with correlation: 0.9019\n",
      "Epoch 4/15, Loss: 0.2633, Val Pearson Corr: 0.9271\n",
      "New best model saved with correlation: 0.9271\n",
      "Epoch 5/15, Loss: 0.2021, Val Pearson Corr: 0.9396\n",
      "New best model saved with correlation: 0.9396\n",
      "Epoch 6/15, Loss: 0.2045, Val Pearson Corr: 0.9509\n",
      "New best model saved with correlation: 0.9509\n",
      "Epoch 7/15, Loss: 0.1721, Val Pearson Corr: 0.9554\n",
      "New best model saved with correlation: 0.9554\n",
      "Epoch 8/15, Loss: 0.1977, Val Pearson Corr: 0.9545\n",
      "Epoch 9/15, Loss: 0.1437, Val Pearson Corr: 0.9578\n",
      "New best model saved with correlation: 0.9578\n",
      "Epoch 10/15, Loss: 0.1559, Val Pearson Corr: 0.9621\n",
      "New best model saved with correlation: 0.9621\n",
      "Epoch 11/15, Loss: 0.1218, Val Pearson Corr: 0.9639\n",
      "New best model saved with correlation: 0.9639\n",
      "Epoch 12/15, Loss: 0.1161, Val Pearson Corr: 0.9639\n",
      "Epoch 13/15, Loss: 0.1188, Val Pearson Corr: 0.9649\n",
      "New best model saved with correlation: 0.9649\n",
      "Epoch 14/15, Loss: 0.1199, Val Pearson Corr: 0.9708\n",
      "New best model saved with correlation: 0.9708\n",
      "Epoch 15/15, Loss: 0.1041, Val Pearson Corr: 0.9738\n",
      "New best model saved with correlation: 0.9738\n",
      "\n",
      "--- Training Complete ---\n",
      "Overall OOF Pearson Correlation: 0.9733\n",
      "Final test predictions aggregated.\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Custom Dataset for efficient loading\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, features, labels=None):\n",
    "        # Convert CuPy arrays to PyTorch tensors and move to device\n",
    "        self.features = torch.from_numpy(cp.asnumpy(features)).to(device)\n",
    "        self.labels = torch.from_numpy(cp.asnumpy(labels)).to(device) if labels is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return self.features[idx], self.labels[idx]\n",
    "        return self.features[idx]\n",
    "\n",
    "# Define the Neural Network Architecture\n",
    "class CryptoPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim=1):\n",
    "        super(CryptoPredictor, self).__init__()\n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(hidden_dims[0]))\n",
    "        layers.append(nn.Dropout(0.2)) # Regularization\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dims[i+1]))\n",
    "            layers.append(nn.Dropout(0.2)) # Regularization\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Model Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    best_val_corr = -1.0 # Pearson correlation as evaluation metric\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                val_preds.extend(outputs.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate Pearson correlation\n",
    "        val_corr, _ = pearsonr(val_true, val_preds)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Pearson Corr: {val_corr:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_corr > best_val_corr:\n",
    "            best_val_corr = val_corr\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"New best model saved with correlation: {best_val_corr:.4f}\")\n",
    "        \n",
    "        model.train()\n",
    "    return model\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dims = [512, 256, 128] # Example: Adjust based on experimentation and RAM\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 2048 # Larger batch size leverages GPU more but consumes more RAM\n",
    "num_epochs = 15 # Adjust based on convergence\n",
    "\n",
    "# K-Fold Cross-Validation for robust evaluation and prediction\n",
    "N_SPLITS = 5\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_predictions = cp.zeros(len(y_train), dtype=cp.float32)\n",
    "test_predictions = cp.zeros((N_SPLITS, len(X_test_scaled)), dtype=cp.float32)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    train_dataset = CryptoDataset(X_train_fold, y_train_fold)\n",
    "    val_dataset = CryptoDataset(X_val_fold, y_val_fold)\n",
    "    test_dataset = CryptoDataset(X_test_scaled) # No labels for test set\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0) # num_workers=0 for Kaggle GPU\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = CryptoPredictor(input_dim, hidden_dims).to(device)\n",
    "    # Optionally use float16 for reduced memory usage (requires compatible GPU and PyTorch version)\n",
    "    # model.half() \n",
    "    \n",
    "    criterion = nn.MSELoss() # Or HuberLoss for robustness to outliers\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "    # Load the best model weights for prediction\n",
    "    trained_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    trained_model.eval() # Set to evaluation mode\n",
    "\n",
    "    fold_test_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            # if model.network[0].weight.dtype == torch.float16:\n",
    "            #    inputs = inputs.half() # Match input type if model is half()\n",
    "            outputs = trained_model(inputs).squeeze()\n",
    "            fold_test_preds.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    test_predictions[fold] = cp.asarray(fold_test_preds, dtype=cp.float32)\n",
    "\n",
    "    # OOF predictions\n",
    "    fold_oof_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in val_loader: # Use val_loader for OOF predictions\n",
    "            inputs = inputs.to(device)\n",
    "            # if model.network[0].weight.dtype == torch.float16:\n",
    "            #    inputs = inputs.half()\n",
    "            outputs = trained_model(inputs).squeeze()\n",
    "            fold_oof_preds.extend(outputs.cpu().numpy())\n",
    "    oof_predictions[val_idx] = cp.asarray(fold_oof_preds, dtype=cp.float32)\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "\n",
    "# Aggregate OOF predictions\n",
    "oof_corr, _ = pearsonr(cp.asnumpy(y_train), cp.asnumpy(oof_predictions))\n",
    "print(f\"Overall OOF Pearson Correlation: {oof_corr:.4f}\")\n",
    "\n",
    "# Average test predictions across folds for final submission\n",
    "final_test_predictions = cp.mean(test_predictions, axis=0)\n",
    "\n",
    "print(\"Final test predictions aggregated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d89a0",
   "metadata": {
    "papermill": {
     "duration": 0.006691,
     "end_time": "2025-07-21T02:32:55.942672",
     "exception": false,
     "start_time": "2025-07-21T02:32:55.935981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd720dd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-21T02:32:55.954929Z",
     "iopub.status.busy": "2025-07-21T02:32:55.954386Z",
     "iopub.status.idle": "2025-07-21T02:32:56.969961Z",
     "shell.execute_reply": "2025-07-21T02:32:56.969067Z"
    },
    "papermill": {
     "duration": 1.022983,
     "end_time": "2025-07-21T02:32:56.971199",
     "exception": false,
     "start_time": "2025-07-21T02:32:55.948216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' created successfully!\n",
      "   ID  prediction\n",
      "0   1   -0.485922\n",
      "1   2    0.165249\n",
      "2   3    0.817161\n",
      "3   4   -0.040804\n",
      "4   5   -0.737516\n"
     ]
    }
   ],
   "source": [
    "# Load sample submission to get the correct format\n",
    "sample_submission_df = pd.read_csv(\"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\")\n",
    "\n",
    "# Ensure test_predictions is a NumPy array for pandas DataFrame creation\n",
    "final_test_predictions_np = cp.asnumpy(final_test_predictions)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({'ID': sample_submission_df['ID'], 'prediction': final_test_predictions_np})\n",
    "\n",
    "# Ensure the label column has the correct data type (float32 if required)\n",
    "submission_df['prediction'] = submission_df['prediction'].astype(np.float32)\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully!\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12993472,
     "sourceId": 96164,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 498.695323,
   "end_time": "2025-07-21T02:33:01.105205",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-21T02:24:42.409882",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
